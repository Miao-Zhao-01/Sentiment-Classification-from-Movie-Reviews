{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.library['ggplot']\n",
    "# 'axes.prop_cycle': cycler('color', ['#E24A33', '#348ABD', '#988ED5', '#777777', '#FBC15E', '#8EBA42', '#FFB5B8']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "dataset_sentence = pd.read_csv('data/datasetSentences.txt', delimiter='\\t')  # sentence_index - sentence\n",
    "dictionary = pd.read_csv('data/dictionary.txt', delimiter='|', header=None)  # phrase - phrase ids\n",
    "sentiment_labels= pd.read_csv('data/sentiment_labels.txt', delimiter='|')  # phrase ids - sentiment values\n",
    "dataset_split= pd.read_csv('data/datasetSplit.txt', delimiter = ',')  # sentence_index - splitset_label\n",
    "\n",
    "dictionary=dictionary.rename(columns={0:'phrase',1:'phrase ids'})\n",
    "sentiment_labels['sentiment']=sentiment_labels['sentiment values'].apply(lambda x: 5 if x>0.8 \n",
    "                                     else 4 if x>0.6\n",
    "                                     else 3 if x>0.4\n",
    "                                     else 2 if x>0.2\n",
    "                                     else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11286 entries, 0 to 11854\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   splitset_label    11286 non-null  int64  \n",
      " 1   phrase            11286 non-null  object \n",
      " 2   sentiment values  11286 non-null  float64\n",
      " 3   sentiment         11286 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 440.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# merge data\n",
    "dataset_label = pd.merge(dataset_sentence, dataset_split)  # sentence_index - sentence - splitset_label\n",
    "phrase_label = pd.merge(dictionary, sentiment_labels)  # phrase - phrase ids - sentiment values - sentiment\n",
    "dataset = pd.merge(dataset_label, phrase_label, how='left', left_on='sentence', right_on='phrase')\n",
    "dataset = dataset.drop(columns=['sentence_index','sentence','phrase ids'])\n",
    "dataset = dataset.dropna(axis=0, how='any')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 8117, valid_set: 1044, test_set: 2125\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "train_set=dataset[dataset['splitset_label']==1]\n",
    "valid_set=dataset[dataset['splitset_label']==3]\n",
    "test_set=dataset[dataset['splitset_label']==2]\n",
    "print('train_set: {}, valid_set: {}, test_set: {}'.format(train_set.shape[0], valid_set.shape[0], test_set.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract phrases\n",
    "SOStr = []\n",
    "STree = []\n",
    "with open('data/SOStr.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        SOStr.append(line.strip().split('|'))\n",
    "with open('data/STree.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        STree.append(list(map(int, line.strip().split('|'))))\n",
    "\n",
    "def get_indices(_list, _value):\n",
    "    indices = []\n",
    "    for idx, value in enumerate(_list):\n",
    "        if value == _value:\n",
    "            indices.append(idx+1)  # +1 beacause the n0 is the root\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_parse_tree(tokens, parents, cur_tree, cur_key, cur_value):\n",
    "    if cur_value <= len(tokens):  # leaf node\n",
    "        cur_tree[cur_key] = tokens[cur_value - 1]\n",
    "    else:  # sub_tree\n",
    "        [left_value, right_value] = get_indices(parents, cur_value)\n",
    "        cur_tree[cur_key] = {'left':left_value, 'right':right_value}\n",
    "        get_parse_tree(tokens, parents, cur_tree[cur_key], 'left', left_value)\n",
    "        get_parse_tree(tokens, parents, cur_tree[cur_key], 'right', right_value)\n",
    "\n",
    "\n",
    "\n",
    "# parse_tree = {'root': max(STree[2])}\n",
    "# get_parse_tree(SOStr[2], STree[2], parse_tree, 'root', max(STree[2]))\n",
    "# parse_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offers|that|rare|combination|of|entertainment|and|education|.\n",
    "16|14|13|13|12|10|10|11|17|11|12|15|14|15|16|17|0\n",
    "\n",
    "Effective|but|too-tepid|biopic\n",
    "6|6|5|5|7|7|0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_set['sentiment']\n",
    "valid_label = valid_set['sentiment']\n",
    "test_label = test_set['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorizing(sentence, dim, model):\n",
    "    vec = np.zeros((1, dim))\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        try:  # some words are ignored\n",
    "            vec += model.wv[word].reshape((1, dim))\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    return vec / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=500, alpha=0.025>', 'datetime': '2022-11-16T01:14:34.081481', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 1030807 words, keeping 81 word types\n",
      "collected 81 word types from a corpus of 1139899 raw words and 11286 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 77 unique words (95.06% of original 81, drops 4)', 'datetime': '2022-11-16T01:14:34.164483', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1139893 word corpus (100.00% of original 1139899, drops 6)', 'datetime': '2022-11-16T01:14:34.165482', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 81 items\n",
      "sample=0.001 downsamples 28 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 229088.37602971366 word corpus (20.1%% of prior 1139893)', 'datetime': '2022-11-16T01:14:34.167481', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 77 words and 500 dimensions: 346500 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-11-16T01:14:34.172483', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 3 workers on 77 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-11-16T01:14:34.173482', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 1139899 raw words (229310 effective words) took 0.2s, 1073781 effective words/s\n",
      "EPOCH 1: training on 1139899 raw words (228969 effective words) took 0.2s, 1031722 effective words/s\n",
      "EPOCH 2: training on 1139899 raw words (228788 effective words) took 0.2s, 1112415 effective words/s\n",
      "EPOCH 3: training on 1139899 raw words (229270 effective words) took 0.2s, 1086362 effective words/s\n",
      "EPOCH 4: training on 1139899 raw words (228968 effective words) took 0.2s, 1089394 effective words/s\n",
      "EPOCH 5: training on 1139899 raw words (228623 effective words) took 0.2s, 1093229 effective words/s\n",
      "EPOCH 6: training on 1139899 raw words (229471 effective words) took 0.2s, 1066995 effective words/s\n",
      "EPOCH 7: training on 1139899 raw words (228499 effective words) took 0.2s, 1035741 effective words/s\n",
      "EPOCH 8: training on 1139899 raw words (229629 effective words) took 0.2s, 1029580 effective words/s\n",
      "EPOCH 9: training on 1139899 raw words (229243 effective words) took 0.2s, 1032743 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 11398990 raw words (2290770 effective words) took 2.2s, 1049887 effective words/s', 'datetime': '2022-11-16T01:14:36.355616', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2290770, 11398990)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8117, 500)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizing = 'word2vec'\n",
    "if vectorizing == 'word2vec':\n",
    "    dim = 500\n",
    "    w2v = Word2Vec(vector_size=dim, min_count=10)  #  Ignores all words with total frequency lower than 10.\n",
    "    w2v.build_vocab(dataset['phrase'])\n",
    "    w2v.train(dataset['phrase'], total_examples=w2v.corpus_count, epochs=10)\n",
    "    train_data = np.concatenate([sentence_vectorizing(sentence, dim, w2v) for sentence in train_set['phrase']])\n",
    "    valid_data = np.concatenate([sentence_vectorizing(sentence, dim, w2v) for sentence in valid_set['phrase']])\n",
    "    test_data = np.concatenate([sentence_vectorizing(sentence, dim, w2v) for sentence in test_set['phrase']])\n",
    "else:\n",
    "    if vectorizing == 'top n': \n",
    "        features_selected = []\n",
    "        for i in range(1, 6):\n",
    "            vectorizer = CountVectorizer(max_features=500)\n",
    "            vectorizer.fit(dataset[dataset['sentiment'] == i]['phrase'])\n",
    "            features_selected.extend(vectorizer.vocabulary_.keys())\n",
    "        features_selected = set(features_selected)\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(features_selected)\n",
    "    else:\n",
    "        if vectorizing == 'one hot':\n",
    "            vectorizer = OneHotEncoder()\n",
    "        elif vectorizing == 'word count':\n",
    "            vectorizer = CountVectorizer()  # including stopwords\n",
    "        elif vectorizing == 'tf-idf':\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        elif vectorizing == 'n-gram':\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(3,3))\n",
    "\n",
    "        vectorizer.fit(dataset['phrase'])\n",
    "\n",
    "    train_data = vectorizer.transform(train_set['phrase']).toarray()\n",
    "    valid_data = vectorizer.transform(valid_set['phrase']).toarray()\n",
    "    test_data = vectorizer.transform(test_set['phrase']).toarray()\n",
    " \n",
    "train_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba1780d77958bbdd70a84500d18db52cb28eb7e0cd9642f5c3eb6b3df3cf3895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
